import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# 1. Create a Mock Dataset (Simulating a real CSV)
data = {
    'Genre': ['Action', 'Drama', 'Comedy', 'Action', 'Drama', 'Sci-Fi', 'Comedy', 'Action', 'Drama', 'Sci-Fi'],
    'Director': ['Nolan', 'Spielberg', 'Gerwig', 'Nolan', 'Scorsese', 'Villeneuve', 'Gerwig', 'Snyder', 'Spielberg', 'Villeneuve'],
    'Actor': ['Bale', 'Hanks', 'Robbie', 'DiCaprio', 'De Niro', 'Chalamet', 'Gosling', 'Affleck', 'Streep', 'Zendaya'],
    'Budget_M': [150, 60, 100, 200, 80, 165, 90, 250, 50, 190],
    'Rating': [8.8, 7.5, 7.8, 8.5, 8.2, 8.0, 7.9, 6.5, 8.1, 8.3]
}

df = pd.DataFrame(data)

# 2. Feature Engineering: Encoding Categorical Data
# Computers don't understand names, so we convert them to numbers
X = df.drop('Rating', axis=1)
y = df['Rating']

# Using One-Hot Encoding for Genre, Director, and Actor
X_encoded = pd.get_dummies(X, columns=['Genre', 'Director', 'Actor'])

# 3. Splitting the data (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# 4. Model Building (Random Forest)
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 5. Predictions
y_pred = model.predict(X_test)

# 6. Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("--- Model Performance ---")
print(f"Mean Squared Error: {mse:.4f}")
print(f"R-squared Score: {r2:.4f}")

# Sample Prediction Output
print("\n--- Actual vs Predicted ---")
comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(comparison)
